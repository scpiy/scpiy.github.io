---
layout:     post
titie:			送给初学者的python爬虫教程
subtitle:		python 小白入门 爬虫
date:				2018-08-07
author:			scpi
header-img:	img/post-2019-08-07.jpg
catalog: true
tags:
    - python
    - 爬虫
---



### 送给初学者的python爬虫教程

**在看本片教程之前最好有一点点编程语言基础，如果是python基础就更好了，不需要很多关于python的知识，但希望你至少对下面这些概念有个了解**

* 列表(list)
* 字典(dict)
* 模块(只要知道这是个什么就好，不用太了解)
* 基础的控制语句，比如if，for

如果你对上面我说的东西一头雾水，建议你可以看看[Python教程](https://www.liaoxuefeng.com/wiki/1016959663602400)，廖雪峰老师的python教程，不用都学完，倒不如说只看一点就可以，对python有一点点了解就可以。

这个简单的入门教程会用到如下几个库

* requests
* BeautifulSoup

分别可以在命令行通过如下命令安装(windows)

```
pip install requests
pip install bs4
```

> 顺便说一下，可能有的人用的不是requests库，而是urllib或是其他的什么，简单说一下他们的区别的话，就是requests更年轻，也更方便，更适合新手，还有BeautifulSoup已经迁移到bs4里了，所以可以直接安装bs4，这样就可以使用BeautifulSoup了

然后说一下这两个库的作用，requests库的作用是得到目标网页的内容，可以通过

```python
request.get(url)
```

实现，url就是我们要解析的网页网址

至于BeautifulSoup，是一个十分方便的，可以帮我们在一堆HTML代码里面找到我们想要的内容的库

今天我只是简单用一下，对他们的功能进行简要介绍，你大概了解就好，之后会比较详细的说一下的

我们今天来爬一下最近很火的国漫电影《哪吒》的短评，首先打开对应的网页

![哪吒短评](https://s2.ax1x.com/2019/08/07/e5c0FH.png)

建议使用chrome浏览器，好处太多，重点是方便我们写爬虫，如果不是chrome的话，可以自己百度打开网页源码的方式。

下面的东西对于没有了解过HTML的人来说可能有些不友好，但也不是不能看，如果你真的不会HTML，可以先用我找出来的路径，~~复制粘贴总会吧，~~把重点放在爬虫程序的构建上。

[![e5gvb8.md.png](https://s2.ax1x.com/2019/08/07/e5gvb8.md.png)](https://imgchr.com/i/e5gvb8)

可以看出来我们要找的东西都在class = “short”的span下，知道这个就好办了

```python
import requests
from bs4 import BeautifulSoup

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36"}
#这个headers，你暂时不用管它是什么，之后会说，现在跟着我做就好
url = "https://movie.douban.com/subject/26794435/comments?limit=20&sort=new_score&status=P"
start = 0
#"https://movie.douban.com/subject/26794435/comments?start=0&limit=20&sort=new_score&status=P"
#这是网页本来的url，经过观察发现，每次翻页那个start的值都会加20，可以利用这个机制实现类似翻页的功能

with open("test.txt", 'a', encoding = "utf8") as fp:
    #打开一个文件，with语句可以不用管，就像正常打开一个文件也可以，使用with只是我的习惯
    #先爬5页，大概就是100条内容
    for i in range(5):
        html = requests.get(url, headers = headers, params = {"start": start})
        start += 20
        soup = BeautifulSoup(html.text, "lxml")
        #html.text是获得名为html的对象的内容
        for comment in soup.find_all("span", class_ = "short"):
            fp.write(comment.text + '\n')
        #find_all会返回找到的所有的匹配对象
        #把匹配到的对象(也就是我们要找的东西)的内容写到文件里，完工
```

最好自己写写代码，但也可以复制粘贴我的代码，作为入门的第一课，没必要对自己太严格。

大概就这样了，我甚至没有写函数......

今天简要介绍了下requests与BeautifulSoup，想要学好爬虫不仅仅这两个库要会用，lxml，正则最好都要会，虽然BeautifulSoup，lxml，正则的作用大致一样，但在应用上还是有差异的，这个之后我再说吧。

还有HTML，最好是会一点，不会倒也可以，只要你不觉得写正则麻烦并且自信自己正则过关。也不难，去W3school[HTML教程](https://www.w3school.com.cn/html/index.asp)看半个小时大概就可以了，又不是让你写前端，有个概念就行了。

那么，今天就到这里了。

感谢你的观看，希望能对你有帮助。

如果有自己解决不了的问题可以联系我。
